{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Trains using PyTorch and logs training metrics and weights in TensorFlow event format to the MLflow run's artifact directory. \n",
    "# This stores the TensorFlow events in MLflow for later access using TensorBoard.\n",
    "#\n",
    "# Code based on https://github.com/mlflow/mlflow/blob/master/example/tutorial/pytorch_tensorboard.py.\n",
    "#\n",
    " \n",
    "from __future__ import print_function\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import namedtuple\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.summary\n",
    "# from tensorflow.summary import scalar\n",
    "# from tensorflow.summary import histogram\n",
    "from chardet.universaldetector import UniversalDetector\n",
    " \n",
    "# Create Params dictionary\n",
    "class Params(object):\n",
    "    def __init__(self, batch_size, test_batch_size, epochs, lr, momentum, seed, cuda, log_interval):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.seed = seed\n",
    "        self.cuda = cuda\n",
    "        self.log_interval = log_interval\n",
    " \n",
    "# Configure args\n",
    "args = Params(64, 1000, 10, 0.01, 0.5, 1, True, 200)\n",
    " \n",
    "cuda = not args.cuda and torch.cuda.is_available()\n",
    " \n",
    " \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    " \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    " \n",
    "    def log_weights(self, step):\n",
    "        writer.add_summary(histogram('weights/conv1/weight', model.conv1.weight.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/conv1/bias', model.conv1.bias.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/conv2/weight', model.conv2.weight.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/conv2/bias', model.conv2.bias.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/fc1/weight', model.fc1.weight.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/fc1/bias', model.fc1.bias.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/fc2/weight', model.fc2.weight.data).eval(), step)\n",
    "        writer.add_summary(histogram('weights/fc2/bias', model.fc2.bias.data).eval(), step)\n",
    " \n",
    "model = Model()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    " \n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    " \n",
    "writer = None # Will be used to write TensorBoard events\n",
    " \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "            step = epoch * len(train_loader) + batch_idx\n",
    "            log_scalar('train_loss', loss.data.item(), step)\n",
    "            model.log_weights(step)\n",
    " \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum().item()\n",
    " \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    log_scalar('test_loss', test_loss, step)\n",
    "    log_scalar('test_accuracy', test_accuracy, step)\n",
    " \n",
    "def log_scalar(name, value, step):\n",
    "    \"\"\"Log a scalar value to both MLflow and TensorBoard\"\"\"\n",
    "    writer.add_summary(scalar(name, value).eval(), step)\n",
    "    mlflow.log_metric(name, value, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/n01z3/anaconda3/bin/pip3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/n01z3/anaconda3/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/n01z3/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorflow\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _tflow_select-2.3.0        |              mkl           2 KB\n",
      "    absl-py-0.13.0             |   py38h06a4308_0         178 KB\n",
      "    aiohttp-3.7.4.post0        |   py38h7f8727e_2         553 KB\n",
      "    astor-0.8.1                |   py38h06a4308_0          47 KB\n",
      "    astunparse-1.6.3           |             py_0          17 KB\n",
      "    async-timeout-3.0.1        |   py38h06a4308_0          13 KB\n",
      "    blinker-1.4                |   py38h06a4308_0          23 KB\n",
      "    cachetools-4.2.2           |     pyhd3eb1b0_0          13 KB\n",
      "    certifi-2021.10.8          |   py38h06a4308_0         151 KB\n",
      "    coverage-5.5               |   py38h27cfd23_2         259 KB\n",
      "    gast-0.4.0                 |     pyhd3eb1b0_0          13 KB\n",
      "    google-auth-1.33.0         |     pyhd3eb1b0_0          80 KB\n",
      "    google-auth-oauthlib-0.4.4 |     pyhd3eb1b0_0          18 KB\n",
      "    google-pasta-0.2.0         |     pyhd3eb1b0_0          46 KB\n",
      "    grpcio-1.36.1              |   py38h2157cd5_1         1.9 MB\n",
      "    keras-preprocessing-1.1.2  |     pyhd3eb1b0_0          35 KB\n",
      "    libprotobuf-3.17.2         |       h4ff587b_1         2.0 MB\n",
      "    markdown-3.3.4             |   py38h06a4308_0         129 KB\n",
      "    multidict-5.1.0            |   py38h27cfd23_2          68 KB\n",
      "    oauthlib-3.1.1             |     pyhd3eb1b0_0          90 KB\n",
      "    opt_einsum-3.3.0           |     pyhd3eb1b0_1          57 KB\n",
      "    protobuf-3.17.2            |   py38h295c915_0         325 KB\n",
      "    pyasn1-0.4.8               |     pyhd3eb1b0_0          54 KB\n",
      "    pyasn1-modules-0.2.8       |             py_0          72 KB\n",
      "    pyjwt-2.1.0                |   py38h06a4308_0          32 KB\n",
      "    python-flatbuffers-2.0     |     pyhd3eb1b0_0          34 KB\n",
      "    requests-oauthlib-1.3.0    |             py_0          23 KB\n",
      "    rsa-4.7.2                  |     pyhd3eb1b0_1          28 KB\n",
      "    tensorboard-2.4.0          |     pyhc547734_0         8.8 MB\n",
      "    tensorboard-plugin-wit-1.6.0|             py_0         630 KB\n",
      "    tensorflow-2.4.1           |mkl_py38hb2083e0_0           4 KB\n",
      "    tensorflow-base-2.4.1      |mkl_py38h43e0292_0        89.0 MB\n",
      "    tensorflow-estimator-2.6.0 |     pyh7b7c402_0         267 KB\n",
      "    termcolor-1.1.0            |   py38h06a4308_1           9 KB\n",
      "    typing-extensions-3.10.0.2 |       hd3eb1b0_0          12 KB\n",
      "    yarl-1.6.3                 |   py38h27cfd23_0         136 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       105.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _tflow_select      pkgs/main/linux-64::_tflow_select-2.3.0-mkl\n",
      "  absl-py            pkgs/main/linux-64::absl-py-0.13.0-py38h06a4308_0\n",
      "  aiohttp            pkgs/main/linux-64::aiohttp-3.7.4.post0-py38h7f8727e_2\n",
      "  astor              pkgs/main/linux-64::astor-0.8.1-py38h06a4308_0\n",
      "  astunparse         pkgs/main/noarch::astunparse-1.6.3-py_0\n",
      "  async-timeout      pkgs/main/linux-64::async-timeout-3.0.1-py38h06a4308_0\n",
      "  blinker            pkgs/main/linux-64::blinker-1.4-py38h06a4308_0\n",
      "  cachetools         pkgs/main/noarch::cachetools-4.2.2-pyhd3eb1b0_0\n",
      "  coverage           pkgs/main/linux-64::coverage-5.5-py38h27cfd23_2\n",
      "  gast               pkgs/main/noarch::gast-0.4.0-pyhd3eb1b0_0\n",
      "  google-auth        pkgs/main/noarch::google-auth-1.33.0-pyhd3eb1b0_0\n",
      "  google-auth-oauth~ pkgs/main/noarch::google-auth-oauthlib-0.4.4-pyhd3eb1b0_0\n",
      "  google-pasta       pkgs/main/noarch::google-pasta-0.2.0-pyhd3eb1b0_0\n",
      "  grpcio             pkgs/main/linux-64::grpcio-1.36.1-py38h2157cd5_1\n",
      "  keras-preprocessi~ pkgs/main/noarch::keras-preprocessing-1.1.2-pyhd3eb1b0_0\n",
      "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.17.2-h4ff587b_1\n",
      "  markdown           pkgs/main/linux-64::markdown-3.3.4-py38h06a4308_0\n",
      "  multidict          pkgs/main/linux-64::multidict-5.1.0-py38h27cfd23_2\n",
      "  oauthlib           pkgs/main/noarch::oauthlib-3.1.1-pyhd3eb1b0_0\n",
      "  opt_einsum         pkgs/main/noarch::opt_einsum-3.3.0-pyhd3eb1b0_1\n",
      "  protobuf           pkgs/main/linux-64::protobuf-3.17.2-py38h295c915_0\n",
      "  pyasn1             pkgs/main/noarch::pyasn1-0.4.8-pyhd3eb1b0_0\n",
      "  pyasn1-modules     pkgs/main/noarch::pyasn1-modules-0.2.8-py_0\n",
      "  pyjwt              pkgs/main/linux-64::pyjwt-2.1.0-py38h06a4308_0\n",
      "  python-flatbuffers pkgs/main/noarch::python-flatbuffers-2.0-pyhd3eb1b0_0\n",
      "  requests-oauthlib  pkgs/main/noarch::requests-oauthlib-1.3.0-py_0\n",
      "  rsa                pkgs/main/noarch::rsa-4.7.2-pyhd3eb1b0_1\n",
      "  tensorboard        pkgs/main/noarch::tensorboard-2.4.0-pyhc547734_0\n",
      "  tensorboard-plugi~ pkgs/main/noarch::tensorboard-plugin-wit-1.6.0-py_0\n",
      "  tensorflow         pkgs/main/linux-64::tensorflow-2.4.1-mkl_py38hb2083e0_0\n",
      "  tensorflow-base    pkgs/main/linux-64::tensorflow-base-2.4.1-mkl_py38h43e0292_0\n",
      "  tensorflow-estima~ pkgs/main/noarch::tensorflow-estimator-2.6.0-pyh7b7c402_0\n",
      "  termcolor          pkgs/main/linux-64::termcolor-1.1.0-py38h06a4308_1\n",
      "  typing-extensions  pkgs/main/noarch::typing-extensions-3.10.0.2-hd3eb1b0_0\n",
      "  yarl               pkgs/main/linux-64::yarl-1.6.3-py38h27cfd23_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                       2021.7.5-h06a4308_1 --> 2021.10.26-h06a4308_2\n",
      "  certifi                          2021.5.30-py38h06a4308_0 --> 2021.10.8-py38h06a4308_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f90ce764829677676aa7565031f144d8302ede72b1ff36dfe6f56a0b00a6b71c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
